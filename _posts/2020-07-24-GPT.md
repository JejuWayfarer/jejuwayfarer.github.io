---
title: GPT 논문 리뷰
date: 2020-07-24
category: NLP
---
# GPT 논문 리뷰

## 1. Introduction
unlabeled text에서 word-level 이상의 정보를 얻는 게 쉽지 않음
이유 - 목적 함수를 알기 힘듬, 학습된 표상을 목표된 taks에 효율적인 전이 방법이 합의 되지 않았음.
그래서 준지도학습으로 접근하기가 어려움
=> combination of unsupervised pre-training and supervised fine-tuning
대용량 코퍼스로 사전 학습 학습을 진행한 후 각각의 task에 맞는 adaptation을 시킬 것
(여기서는 BERT와의 차이점이 없는 건가?)


## 2. Related Work
  - Semi-supervised learning for NLP
    임베딩의 경우 단어 단위의 정보는 많이 포함하지만 상위 단위의 의미는 많이 포함하지 못함.
  - Unsupervised pre-training
    최근에는 이미지 분류, 음성 인식, 개체명인식, 기계번역에 사용
    LSTM을 사용하는 사전 학습 과정이 어느 정도의 언어 정보를 포함하지만 짧은 범위의 예측 능력을 떨어뜨림
    이러한 한계를 Transformer의 사용으로 극복하고자 함.


## 3. Framework

방대한 코퍼스로 마든 대용량 모형 학습 => labeles data 판별을 위한 fine-tuning

  ### 3-1 Unsupervise pre-training(비지도 사전학습)
  ![objective function](https://user-images.githubusercontent.com/49282663/88509525-37ad7e00-d01c-11ea-93e4-8e4598e9badd.png)
  - multi-layer Transformer decoder
  ![image](https://user-images.githubusercontent.com/49282663/88510610-5876d300-d01e-11ea-8f1e-223f566846ff.png)
  
  ### 3-2 Supervised fine-tuning(지도 미세조정)
  ![objective function](https://user-images.githubusercontent.com/49282663/88510475-059d1b80-d01e-11ea-8d94-46b7dffde49c.png)
  ![Transformer architecture and training objectives](https://user-images.githubusercontent.com/49282663/88510541-2ebdac00-d01e-11ea-86cf-c96df82b9b42.png)
  - 1~m까지의 단어가 주어졌을 때 y의 확률이 최대가 되는 것을 찾는다.
  ![finetuning task](https://user-images.githubusercontent.com/49282663/88510660-6fb5c080-d01e-11ea-889f-8002743397e3.png)
  

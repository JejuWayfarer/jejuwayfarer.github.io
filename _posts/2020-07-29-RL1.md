---
title: 강화학습 MDP와 벨만방정식
date: 2020-07-29
category: RL
---

# 1. MDP(Markov Decision Process)
강화학습은 순차적을 핻동을 계속 결정해야 하는 문제를 푸는 것이다.

![식1](https://user-images.githubusercontent.com/49282663/88745419-f7b5da80-d184-11ea-8889-e7396805b724.png)
![식2](https://user-images.githubusercontent.com/49282663/88745443-0ac8aa80-d185-11ea-87e8-754dcca344f7.png)

식1과 식2가 같을 때 markov한 상황으로 정의할 수 있다. 즉 현재 상태가 될 이전 state의 영향만 받는 다고 가정하는 것이다.

### MDP(Markov Decision Process)는 상태(state), 행동(action), 상태변환확률(state transition probability matrix), 보상(reward), 할인율(discount factor)로 이루어져 있다./
![MDP](https://user-images.githubusercontent.com/49282663/88746113-b6262f00-d186-11ea-906b-2ae2fa33be1c.png)

### State- agent가 인식하는 자신의 상태 St는 r.v.(확률변수)

### Action- agent가 action을 통해서 state를 변화. St에서 할 수 있는 가능한 행동의 집합. At는 r.v.

### state transition probability matrix- s라는 state에서 a라는 행동을 취할 대 s'에 도착할 확률
![상태변환확률](https://user-images.githubusercontent.com/49282663/88746179-dd7cfc00-d186-11ea-8d53-87f4bc10d6d9.png)

### Reward- agent가 학습할 수 있는 유일한 정보. 환경이 에이전트에게 주는 정보.
![image](https://user-images.githubusercontent.com/49282663/88746499-ab1fce80-d187-11ea-9231-1730b94cc3b7.png)

### Discount Factor- 현재의 보상을 미래의 보상보다 선호하기 때문에 미래의 보상에 할인율을 곱해서 계산. 0~1의 값을 가짐

### Policy- 모든 상태에서 agent가 할 행동. agent가 학습을 통해서 최적 정책을 학습.
![Policy](https://user-images.githubusercontent.com/49282663/88746786-47e26c00-d188-11ea-9c39-9f063d798e3a.png)
St = s일 때 At=a일 확률



# 2. Value Function(가치 함수)

### return- t 시점 이후의 t 시점에서의 보상을 반환값 G(return). Gt는 r.v.
![image](https://user-images.githubusercontent.com/49282663/88747293-71e85e00-d189-11ea-9a6b-a1bec1082c1d.png)

### (state) value function
agent는 반환값에 대한 기대값으로 특정 상태의 가치를 판단. St=s에서 return에 대한 기대값
![image](https://user-images.githubusercontent.com/49282663/88747828-8bd67080-d18a-11ea-8f3d-30f6024f44e9.png)
가치함수는 앞으로 받을 보상에 대한 기대값인 가치함수로 표현 가능. 또한 특정 상황에서 선택할 행동과 상태 변환 확률을 고려하기 위해서 정책을 고려해야 ㅎ낟.
![정책을 고려한 가치함수](https://user-images.githubusercontent.com/49282663/88748066-1919c500-d18b-11ea-8206-876406c9df6d.png)
상태가 입력으로 들어오면 그 상태에서 앞으로 받을 보상의 합을 출력

### 큐함수 (action-value function)
agent가 선택한 행동에 따라 즉각적으로 받는 보상이 달라짐. 또한 행동을 했더라도 상태 변환 확률에 따라 행동하지 state가 변화하지 않을 수도 있음.
상태 가치함수가 상태에 대해 가치를 알려주는 것처럼 행동에 대한 기대값을 알려주는 것이 행동 가치함수
![image](https://user-images.githubusercontent.com/49282663/88748587-5894e100-d18c-11ea-8531-ace443673b23.png)
정책이 π, St=s일 때 a라는 행동을 취했을 때의 return에 대한 기대값.

![image](https://user-images.githubusercontent.com/49282663/88756738-5be59800-d19f-11ea-9535-b3646920f278.png)가치 함수는 정책과 큐함수를 이용해 나타낼 수 있다.


# 3. Bellman Expectaiotn Equation(벨만 기대 방정식)
Bellman Equation: 현재 상태의 가치함수와 다음 상태의 가치함수의 관계를 식으로 나타낸 것
![Bellman Equation](https://user-images.githubusercontent.com/49282663/88756021-64d56a00-d19d-11ea-984c-321ace951229.png)
위의 벨만 기대 방정식으로 계속 업데이트하는 방식으로 최적의 기댓값을 구할 수 있게 된다.

![큐함수의 벨만 기대 방정식](https://user-images.githubusercontent.com/49282663/88756029-69018780-d19d-11ea-81ce-34878f0cdc8a.png)

기댓값을 계산하기 위해서는 정책과 상태 변환 확률을 포함한 식으로 형태를 바꾸게 되면 아래와 같이 된다.

![image](https://user-images.githubusercontent.com/49282663/88757190-6fddc980-d1a0-11ea-94ae-0d3731f6fd0a.png)
 
action-value function도 위와 같은 형태로 만들 수 있다.



Reference
https://dnddnjs.gitbooks.io/rl/content/
파이썬과 케라스로 배우는 강화학습_이웅원 외_위키북스

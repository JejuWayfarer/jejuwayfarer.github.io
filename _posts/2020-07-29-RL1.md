---
title: [강화학습] MDP와 벨만방정식
date: 2020-07-29
category: RL
---
이 게시물은 
"https://dnddnjs.gitbooks.io/rl/content/"

"파이썬과 케라스로 배우는 강화학습_이웅원 외_위키북스"

두 출처의 내용을 공부를 위한 목적으로 가공 편집한 것입니다.
문제가 된다면 즉각 삭제하겠습니다.
  
# 1. MDP(Markov Decision Process)
강화학습은 순차적을 핻동을 계속 결정해야 하는 문제를 푸는 것이다.

![식1](https://user-images.githubusercontent.com/49282663/88745419-f7b5da80-d184-11ea-8889-e7396805b724.png)

![식2](https://user-images.githubusercontent.com/49282663/88745443-0ac8aa80-d185-11ea-87e8-754dcca344f7.png)

위의 두 식이 같을 때 markov한 상황으로 정의할 수 있다. 즉 현재 상태가 될 이전 state의 영향만 받는다고 가정하는 것이다.

## MDP(Markov Decision Process)
상태(state), 행동(action), 상태변환확률(state transition probability matrix), 보상(reward), 할인율(discount factor)로 이루어짐

![MDP](https://user-images.githubusercontent.com/49282663/88746113-b6262f00-d186-11ea-906b-2ae2fa33be1c.png)

### State
agent가 인식하는 자신의 상태 St는 r.v.(확률변수)

### Action
agent가 action을 통해서 state를 변화. St에서 할 수 있는 가능한 행동의 집합. At는 r.v.

### state transition probability matrix
s라는 state에서 a라는 행동을 취할 대 s'에 도착할 확률
![상태변환확률](https://user-images.githubusercontent.com/49282663/88746179-dd7cfc00-d186-11ea-8d53-87f4bc10d6d9.png)

### Reward
agent가 학습할 수 있는 유일한 정보. 환경이 에이전트에게 주는 정보.
![image](https://user-images.githubusercontent.com/49282663/88746499-ab1fce80-d187-11ea-9231-1730b94cc3b7.png)

### Discount Factor: r
현재의 보상을 미래의 보상보다 선호하기 때문에 미래의 보상에 할인율을 곱해서 계산. 0~1의 값을 가짐

### Policy- 모든 상태에서 agent가 할 행동. agent가 학습을 통해서 최적 정책을 학습.
![Policy](https://user-images.githubusercontent.com/49282663/88746786-47e26c00-d188-11ea-9c39-9f063d798e3a.png)
St = s일 때 At=a일 확률




# 2. Value Function(가치 함수)

### Return(반환값)
t시점 이후의 t 시점에서의 보상 Gt는 r.v.
![image](https://user-images.githubusercontent.com/49282663/88747293-71e85e00-d189-11ea-9a6b-a1bec1082c1d.png)

### (state) value function (가치 함수)
agent는 반환값에 대한 기대값으로 특정 상태의 가치를 판단. St=s에서 return에 대한 기대값
![image](https://user-images.githubusercontent.com/49282663/88747828-8bd67080-d18a-11ea-8f3d-30f6024f44e9.png)

가치함수는 앞으로 받을 보상에 대한 기대값. 특정 상황에서 선택할 행동과 상태 변환 확률을 고려하기 위해서 정책을 고려.

![정책을 고려한 가치함수](https://user-images.githubusercontent.com/49282663/88748066-1919c500-d18b-11ea-8206-876406c9df6d.png)

상태가 입력으로 들어오면 그 상태에서 앞으로 받을 보상의 합을 출력


### 큐함수 (action-value function)
agent가 선택한 행동에 따라 즉각적으로 받는 보상이 달라짐. 행동을 했더라도 상태 변환 확률에 따라 행동하지 state가 변화하지 않을 수도 있다는 것을 고려.

상태 가치함수가 상태에 대해 가치를 알려주는 것처럼 행동에 대한 기대값을 알려줌.

![image](https://user-images.githubusercontent.com/49282663/88748587-5894e100-d18c-11ea-8531-ace443673b23.png)

정책이 π, St=s일 때 a라는 행동을 취했을 때의 return에 대한 기대값

![image](https://user-images.githubusercontent.com/49282663/88756738-5be59800-d19f-11ea-9535-b3646920f278.png)

가치 함수는 정책과 큐함수를 이용해 나타낼 수 있다.


# 3. Bellman Expectaiotn Equation(벨만 기대 방정식)
Bellman Equation: 현재 상태의 가치함수와 다음 상태의 가치함수의 관계를 식으로 나타낸 것

![Bellman Equation](https://user-images.githubusercontent.com/49282663/88756021-64d56a00-d19d-11ea-984c-321ace951229.png)
![큐함수의 벨만 기대 방정식](https://user-images.githubusercontent.com/49282663/88756029-69018780-d19d-11ea-81ce-34878f0cdc8a.png)

기댓값을 계산하기 위해서 정책과 상태 변환 확률을 포함한 식으로 형태를 바꾸게 되면 아래와 같이 된다.

![image](https://user-images.githubusercontent.com/49282663/88757917-21c9c580-d1a2-11ea-82dd-f28065e1aed2.png)
![image](https://user-images.githubusercontent.com/49282663/88757190-6fddc980-d1a0-11ea-94ae-0d3731f6fd0a.png)
 
위의 벨만 기대 방정식으로 계속 업데이트하는 방식으로 최적의 기댓값을 구할 수 있게 된다.
강화학습의 목적은 최적 정책을 찾는 것, 즉 accumilative future reward를 최대로 하는 policy를 찾는 것이다. 이는 가치함수의 최대값을 찾는 방식으로 알 수 있다.
가장 높은 가치함수에서 최적 정책은 각 상태 s에서의 가장 큰 큐함수를 구하는 것과 같다.
따라서 최종적인 목표는 optimal Q-funtion을 구하는 것이 된다.

![image](https://user-images.githubusercontent.com/49282663/88758408-5722e300-d1a3-11ea-9b14-4d1c4ed91e95.png)
![image](https://user-images.githubusercontent.com/49282663/88758554-ad902180-d1a3-11ea-8f9a-baf1e12d400e.png)

![image](https://user-images.githubusercontent.com/49282663/88758782-42931a80-d1a4-11ea-8006-e136b79cf858.png)

벨만 기대 방정식과 벨만 최적 방정식을 이용해 MDP 문제를 푸는 방법이 다이나믹 



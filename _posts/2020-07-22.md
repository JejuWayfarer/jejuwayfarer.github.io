---
title: "RoBERTa 논문 리뷰"
date: 2020-07-22
categories: NLP
---
RoBERTa 논문 리뷰

1. 요약

  (1) 더 큰 배치, 더 많은 데이터로 더 길게 학습시킴
  (2) NSP 제거
  (3) seq_len을 더욱 길게
  (4) masking pattern을 변화(dynamic masking)

  성과 : MNLI, QNLI, RTE, STS-B, SQuAD, RACE에서 SOTA 달성.

2. 기존 BERT model

  2.3 Training Objectives
    -MLM: 전체의 15% masking, masking의 80% [MASK], 10% 그대로, 10% 다른 단어로 대체.
    -NSP: 50% 연속된 문장 50% 다른 문서에서. NLI(natural language inference) task의 퍼포먼스 향상을 위해

  2.4 Optimization
    Adam: parameters β1 = 0.9 β2 = 0.999 ǫ = 1e-6 L2 weight decay of 0.01. 
    The learning rate is warmed up over the first 10,000 steps to a peak value of 1e-4, and then linearly decayed. 
    Trains with a dropout of 0.1 on all layers and attention weights, and a GELU activation function 
    Models are pretrained for S = 1,000,000 updates, with minibatches containing B = 256 sequences of maximum length T = 512 tokens.
  
  2.5 Data - BOOKCORPUS + Eng Wiki(total 15GB)

3. 방법론적 탐색

  3.2 Data - BOOKCORPUS, CC-NEWS, OPENWEBTEXT, STORIES (Total over 160GB)
    텍스트의 양이 많아질수록 end-task의 성능이 향상

  3.3 Evaluation - GLUE, SQuAD, RACE


4. 학습 과정 분석
begin: BERTBASE (L = 12, H = 768, A = 12, 110M params)
  4.1 Static vs. Dynamic Masking => 소폭의 성능 향상이 있어 동적 마스킹 결정
  4.2 NSP 여부 => 결론적으로 제외
  4.3 mini-batch sizes: 매우 큰 mini-batches가 최적화와 end-task performance 모두에 뛰어남. 미니배치가 크면 MLM의 perplexity, end-task Perf가 향상.
  
 
 
5. RoBERTa (Robustly optimizer BERT approah): dynamic maksing, FULL-SENTENCES without NSP, large mini-batches, laeger byre-level BPE
    BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters)로 시작
    RoBERTa가 SQuAD, MNLI-m, STT-2, GLUE(9개, single-taksk vsingle model)에서 SOTA 달성 => model architecture or pretraining objective 보다 dataset size나 training time이 중요


6. 결론
큰 배치와 많은 데이터를 이용해 더 오래 학습시켰더니 더욱 좋은 결과가 나옴




cf) GLUE datasets
MNLI(Multi-Genre Natural Language Inference): entailment classification task
QQP(Quora Question Pairs): Quora에 올라온 질문 페어가 의미적으로 동일한지 확인하는 테스크
QNLI(Question Natural Language Inference): SQuAD의 이진분류 버전. paragraph가 answer를 포함하는지 안하는지 확인하는 문제.
SST-2(Stanford Sentiment Treebank): 단문장 이진분류문제. 영화리뷰에서 추출된 문장에 감정이 표기되어있음.
CoLA(Corpus of Linguistic Acceptability): 영어문장이 언어학적으로 acceptable한지 확인하는 이진분류문제
STS-B(Seemantic Textual Similarity Benchmark): 문장쌍이 얼마나 유사한지 확인하는 문제.
MRPC(Microsoft Research Paraphrate Corpus): 문장쌍의 유사성 확인하는 문제.
RTE(Recognizing Textual Entailment): MNLI와 유사하나 데이터가 적음.
WNLI(Winograd NLI): 자연어 추론 데이터셋이나 현재 채점에 이슈가 있어서 BERT 실험에서는 제외됨.
출처: https://vanche.github.io/NLP_Pretrained_Model_BERT(2)/


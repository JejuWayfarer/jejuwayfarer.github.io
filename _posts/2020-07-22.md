---
title: ""
date: 2020-07-22
categories: NLP
---
RoBERTa 논문 리뷰

BERT와의 차이점
1) 더 큰 배치, 더 많은 데이터로 더 길게 학습시킴
2) NSP 제거
3) seq_len을 더욱 길게
4) masking pattern을 변화시킴

성과 : MNLI, QNLI, RTE, STS-B, SQuAD, RACE에서 SOTA 달성.

cf) GLUE datasets
MNLI(Multi-Genre Natural Language Inference): entailment classification task
QQP(Quora Question Pairs): Quora에 올라온 질문 페어가 의미적으로 동일한지 확인하는 테스크
QNLI(Question Natural Language Inference): SQuAD의 이진분류 버전. paragraph가 answer를 포함하는지 안하는지 확인하는 문제.
SST-2(Stanford Sentiment Treebank): 단문장 이진분류문제. 영화리뷰에서 추출된 문장에 감정이 표기되어있음.
CoLA(Corpus of Linguistic Acceptability): 영어문장이 언어학적으로 acceptable한지 확인하는 이진분류문제
STS-B(Seemantic Textual Similarity Benchmark): 문장쌍이 얼마나 유사한지 확인하는 문제.
MRPC(Microsoft Research Paraphrate Corpus): 문장쌍의 유사성 확인하는 문제.
RTE(Recognizing Textual Entailment): MNLI와 유사하나 데이터가 적음.
WNLI(Winograd NLI): 자연어 추론 데이터셋이나 현재 채점에 이슈가 있어서 BERT 실험에서는 제외됨.
출처: https://vanche.github.io/NLP_Pretrained_Model_BERT(2)/

